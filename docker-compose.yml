services:
  backend:
    image: vllm/vllm-openai:v0.6.2
    container_name: vllm-backend
    runtime: nvidia
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
      - HUGGING_FACE_HUB_TOKEN=${HUGGING_FACE_HUB_TOKEN}
    ports:
      - "8000:8000"
    ipc: host
    command:
      - "--model=meta-llama/Llama-3.2-3B-Instruct"
      - "--dtype=half"
      - "--tensor_parallel_size=1"
      - "--trust-remote-code"
      - "--max-model-len=4096"
    networks:
      - app-network

  frontend:
    container_name: next-frontend
    build:
      context: ./frontend
      dockerfile: Dockerfile
    depends_on:
      - backend
    ports:
      - "3000:3000"
    env_file:
      - ./frontend/.env
    networks:
      - app-network

  nginx:
    image: nginx:stable-alpine3.20
    container_name: nginx-proxy
    depends_on:
      - frontend
      - backend
    ports:
      - "8080:80"  # Expose Nginx on port 8080
    volumes:
      - ./frontend/nginx.conf:/etc/nginx/conf.d/default.conf
    networks:
      - app-network

networks:
  app-network:
    driver: bridge